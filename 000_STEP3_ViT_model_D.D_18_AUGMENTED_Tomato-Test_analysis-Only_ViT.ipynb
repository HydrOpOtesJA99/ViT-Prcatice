{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a01f9f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T07:24:33.965320Z",
     "start_time": "2023-12-08T07:24:33.947828Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import os\n",
    "\n",
    "# 11.14 additional setup\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from keras import Input, optimizers\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras import layers\n",
    "from keras.applications import ResNet50, DenseNet201, VGG19\n",
    "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB7\n",
    "from random import sample\n",
    "from datetime import datetime\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from keras.layers import BatchNormalization, Conv2D, Activation, Dense, MaxPooling2D, Dropout, Flatten, concatenate\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f262308c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T05:15:10.254573Z",
     "start_time": "2023-12-08T05:15:10.244130Z"
    }
   },
   "outputs": [],
   "source": [
    "print('device_lib.list [1]:', device_lib.list_local_devices()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2117b02f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T05:58:02.413996Z",
     "start_time": "2023-12-08T05:58:02.389250Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure the hyperparameters\n",
    "\n",
    "# Basic parameter\n",
    "Ratio_val_set = 0.2\n",
    "Ratio_test_set = 0.2\n",
    "iter = 3\n",
    "\n",
    "# v2 기준 parameter\n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%Y-%m-%d_%H:%M:%S\") #ref : https://dev-jy.tistory.com/5\n",
    "# es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=5) # early stop을 걸어야 할까?\n",
    "\n",
    "# leaf classification v2 원본(CNN model 5개 비교하는 일 했으므로 (iter, 5)로 해둠)\n",
    "# result_acc = np.zeros(shape=(iter, 5))\n",
    "# result_val_acc = np.zeros(shape=(iter, 5))\n",
    "# result_precision=np.zeros(shape=(iter, 5))\n",
    "# result_recall=np.zeros(shape=(iter, 5))\n",
    "# result_f1=np.zeros(shape=(iter, 5))\n",
    "\n",
    "result_acc = np.zeros(shape=(iter, 1))\n",
    "result_val_acc = np.zeros(shape=(iter, 1))\n",
    "result_precision=np.zeros(shape=(iter, 1))\n",
    "result_recall=np.zeros(shape=(iter, 1))\n",
    "result_f1=np.zeros(shape=(iter, 1))\n",
    "\n",
    "# 저장용 폴더 생성\n",
    "folder = '/002_18_AUGMENTED_STEP3_Dataset-analysis-RESULT'\n",
    "\n",
    "if not (os.path.isdir('/BiO/Sangjin_Ko_work_2022/16_Vision_Transformer_TEST' + folder )):\n",
    "    os.makedirs('/BiO/Sangjin_Ko_work_2022/16_Vision_Transformer_TEST' + folder)\n",
    "\n",
    "writer = pd.ExcelWriter('/BiO/Sangjin_Ko_work_2022/16_Vision_Transformer_TEST' + folder + '/Tomato_Disease_classification_result_'+ str(now) + '.xlsx', engine='xlsxwriter')\n",
    "\n",
    "\n",
    "# 분류 class 개수 (cifar-100 dataset 사용하므로)\n",
    "num_classes = 4 # Healthy or Disease 이므로\n",
    "\n",
    "# input 사진 크기\n",
    "input_shape = (256, 256, 3)\n",
    "\n",
    "# 학습률\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 가중치 감소\n",
    "weight_decay = 0.0001\n",
    "    # Weight_decay = 가중치 감소\n",
    "    # ref: http://www.gisdeveloper.co.kr/?p=8443\n",
    "    # 오버피팅을 해소하기 위해 특정값을 손실함수에 더해 주는 것\n",
    "    # weight_decay 값이 클 수록 가중치 값이 적어져 오버피팅을 해소할 수 있지만,\n",
    "    # weight decay 값을 너무 크게하면 언더피팅현상이 발생할 수 있다\n",
    "\n",
    "# batch : batch 마다 나눠진 데이터 셋의 size 뜻함 데이터 샘플의 size\n",
    "batch_size = 2\n",
    "    # ref: https://m.blog.naver.com/qbxlvnf11/221449297033\n",
    "\n",
    "# epoch : 전체 데이터셋 학습 횟수\n",
    "num_epochs = 100\n",
    "    # ref: https://m.blog.naver.com/qbxlvnf11/221449297033\n",
    "\n",
    "# iteration : epoch를 나눠 실행하는 횟수\n",
    "\n",
    "    # 메모리의 한계와 속도 저하 떄문에 한번의 epoch에서 모든 데이터 넣을 수 없다\n",
    "    # 따라서 데이터를 나눠서 주게 되는데 이때 몇 번 나누는가를 iteration,\n",
    "    # iteration 마다 주는 데이터 사이즈를 batch size라고 한\n",
    "\n",
    "image_size = 128  # We'll resize input images to this size\n",
    "\n",
    "# patch_size\n",
    "patch_size = 8  # Size of the patches to be extract from the input images\n",
    "    # image를 patch size 크기의 픽셀 단위로 자름\n",
    "\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "# 원본 projection dimension (차원 축소 )\n",
    "projection_dim = 64\n",
    "    # ref: chat gpt\n",
    "    # 신경망에 입력된 데이터를 저차원 벡터로 매핑하는 층을 가지는 이때 저차원 벡터의 크기나 차원을 말함\n",
    "\n",
    "\n",
    "num_heads = 4\n",
    "\n",
    "# 원본\n",
    "# transformer_units = [projection_dim * 2,projection_dim,]  # Size of the transformer layers\n",
    "\n",
    "transformer_units = [projection_dim * 2, projection_dim,]  # Size of the transformer layers\n",
    "\n",
    "transformer_layers = 8\n",
    "\n",
    "# 원본\n",
    "mlp_head_units = [2048, 1024] # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc159143",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T05:21:14.119247Z",
     "start_time": "2023-12-08T05:21:12.834525Z"
    }
   },
   "outputs": [],
   "source": [
    "# # google drive에서 raw data 업로드\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# # 분류 class 개수\n",
    "# num_classes = 4 # Normal or Disease 이므로\n",
    "\n",
    "# # input 사진 크기\n",
    "# input_shape = (256, 256, 3)\n",
    "\n",
    "# Potato_folder = '/STEP3_RE_NAME_AUGMENTATION_Tomato_all'\n",
    "Tomato_folder = '/STEP3_RE_NAME_AUGMENTATION_Tomato_all'\n",
    "\n",
    "\n",
    "# Potato_dataset_path = '/BiO/Sangjin_Ko_work_2022/16_Vision_Transformer_TEST/001_RE_NAME_18_AUGMENTED_STEP3_Dataset' + Potato_folder\n",
    "Tomato_dataset_path = '/BiO/Sangjin_Ko_work_2022/16_Vision_Transformer_TEST/001_RE_NAME_18_AUGMENTED_STEP3_Dataset' + Tomato_folder\n",
    "\n",
    "data_generator = ImageDataGenerator(rescale=1./ 255) # 원래는 1./255; 이 내용 없이는 안되나? -> 해당 내용 있어야 이미지 제대로 읽힘\n",
    "\n",
    "\n",
    "# Potato_Dataset_all = data_generator.flow_from_directory(Potato_dataset_path, batch_size=1, color_mode='rgb', class_mode='categorical')\n",
    "Tomato_Dataset_all = data_generator.flow_from_directory(Tomato_dataset_path, batch_size=1, color_mode='rgb', class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a733f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T05:21:20.857939Z",
     "start_time": "2023-12-08T05:21:20.635167Z"
    }
   },
   "outputs": [],
   "source": [
    "## v2 기반 script\n",
    "# Tomato Sample 이름 가져오기\n",
    "Tomato_Sample_info = [] # Tomato or Potato or Pepper\n",
    "for idx_sample_name in range(len(Tomato_Dataset_all.filenames)):\n",
    "    Tomato_Sample_info.append([Tomato_Dataset_all.filenames[idx_sample_name].split('/')[0],\n",
    "                               Tomato_Dataset_all.filenames[idx_sample_name].split('/')[1]])\n",
    "\n",
    "print(Tomato_Dataset_all.filenames[idx_sample_name].split('/')[0])\n",
    "print(Tomato_Dataset_all.filenames[idx_sample_name].split('/')[1])\n",
    "\n",
    "Tomato_Sample_info = [tuple(tmp) for tmp in Tomato_Sample_info]\n",
    "Tomato_Sample_info = list(set(Tomato_Sample_info))\n",
    "Tomato_Sample_info.sort()\n",
    "Tomato_Sample_info = pd.DataFrame(Tomato_Sample_info)\n",
    "\n",
    "Tomato_Sample_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a83962e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T05:21:24.918076Z",
     "start_time": "2023-12-08T05:21:24.910005Z"
    }
   },
   "outputs": [],
   "source": [
    "# Implement multilayer perceptron(MLP)\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "            # tf.nn.gelu : Compute the Gaussian Error Linear Unit (GELU) activation function.\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "901e4e30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T05:22:39.392735Z",
     "start_time": "2023-12-08T05:22:39.380021Z"
    }
   },
   "outputs": [],
   "source": [
    "# Implement patch creation as a layer\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17bfbc1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T05:22:40.369408Z",
     "start_time": "2023-12-08T05:22:40.357734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Implement the patch encoding layer\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)\n",
    "        print('projection_dim:', projection_dim)\n",
    "        print('num_patches:', num_patches)\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        # ####\n",
    "        # print('encoded:', encoded)\n",
    "\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9390261",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T05:31:22.209748Z",
     "start_time": "2023-12-08T05:31:22.188328Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the ViT model\n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # ####\n",
    "    # print('inputs:', inputs)\n",
    "    # print()\n",
    "\n",
    "    # Augment data. 원본스크립트\n",
    "    resized = data_resizing(inputs)\n",
    "\n",
    "    # Create patches.\n",
    "    # patches = Patches(patch_size)(augmented) -> 원본스크립트\n",
    "    patches = Patches(patch_size)(resized)\n",
    "    ####\n",
    "    # print('patches:', patches)\n",
    "    # print(f\"patches:, {patches.shape}\")\n",
    "\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "    ####\n",
    "    # print('encoded_patches:', encoded_patches)\n",
    "    # print(f\"encoded_patches:, {encoded_patches.shape}\")\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        ####\n",
    "        # print('x1:', x1)\n",
    "        # print(f\"x1 shape:, {x1.shape}\")\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)\n",
    "        ####\n",
    "        # print('attention_output:', attention_output)\n",
    "        # print('attention_output shape', attention_output.shape)\n",
    "        # print(f\"Attention Output Shape: {attention_output.shape}\")\n",
    "        # print(f\"Encoded Patches Shape: {encoded_patches.shape}\")\n",
    "\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        ####\n",
    "        # print('x2:', x2)\n",
    "        # print('x2 shape:', x2.shape)\n",
    "        # print(f\"x2 shape: {x2.shape}\")\n",
    "\n",
    "        # # Add reshaping layers if needed\n",
    "        # x2 = layers.Add()([attention_output, layers.Reshape((-1, projection_dim))(encoded_patches)])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        ####\n",
    "        # print('x3:', x3)\n",
    "        # print(f\"x3 shape: {x3.shape}\")\n",
    "\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        ####\n",
    "        # print('x3:', x3)\n",
    "        # print(f\"x3 shape: {x3.shape}\")\n",
    "\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "        ####\n",
    "        # print('encoded_patches:', encoded_patches)\n",
    "        # print(f\"encoded_patches: {encoded_patches}\")\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    ####\n",
    "    # print('representation:', representation)\n",
    "    # print(f\"representation: {representation}\")\n",
    "\n",
    "    representation = layers.Flatten()(representation)\n",
    "    ####\n",
    "    # print('representation:', representation)\n",
    "    # print(f\"representation: {representation}\")\n",
    "\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    ####\n",
    "    # print('representation:', representation)\n",
    "    # print(f\"representation: {representation}\")\n",
    "\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    ####\n",
    "    # print('features:', features)\n",
    "    # print(f\"features: {features}\")\n",
    "\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    ####\n",
    "    # print('logits:', logits)\n",
    "    # print(f\"logits: {logits}\")\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    ####\n",
    "    # print('model:', model)\n",
    "    # print(f\"model: {model}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b07b154",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T05:32:42.869398Z",
     "start_time": "2023-12-08T05:32:42.812797Z"
    }
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# Pepper Potato Tomato 따라서 path 변경해줘야함 ~!!!!!! #\n",
    "#########################################################\n",
    "\n",
    "# Compile, train, and evaluate the mode\n",
    "\n",
    "# for idx_model in range(5):\n",
    "    # 0 -> Resnet50\n",
    "    # 1 -> Alexnet\n",
    "    # 2 -> Googlenet\n",
    "    # 3 -> VGG19\n",
    "    # 4 -> Efficientnet\n",
    "\n",
    "################################################################################################################################################################################################################################################################    \n",
    "idx_model=0 # ViT\n",
    "\n",
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    ####\n",
    "    print('optimizer:', optimizer)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "\n",
    "        # 원본 스크립트\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "\n",
    "        # Categorical Crossentropy 사용한 스크립트 - 에러발생\n",
    "        # loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "\n",
    "        metrics=[\n",
    "            # 원본 스크립트\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            # keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "\n",
    "            # Categorical Crossentropy 사용한 스크립트 - 에러발생\n",
    "            # keras.metrics.CategoricalCrossentropy(name=\"accuracy\"),\n",
    "            # keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "    ################################################################################################################################################################################################################################################################    \n",
    "    \n",
    "    ################################################################################################################################################################################################################################################################\n",
    "    # Pepper Potato or Tomato\n",
    "    if not (os.path.isdir('/BiO/Sangjin_Ko_work_2022/16_Vision_Transformer_TEST' + folder + '/Tomato_Checkpoint/')):\n",
    "        os.makedirs('/BiO/Sangjin_Ko_work_2022/16_Vision_Transformer_TEST' + folder + '/Tomato_Checkpoint/')\n",
    "\n",
    "    checkpoint_filepath = '/BiO/Sangjin_Ko_work_2022/16_Vision_Transformer_TEST' + folder + '/Tomato_Checkpoint/' + 'Tomato_Iteration_' + str(idx_iter) + '_Checkpoint_' + str(now)\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    \n",
    "    with tf.device('/GPU:0'):\n",
    "        history = model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=num_epochs,\n",
    "            verbose=1,  # v2 script에서 참고\n",
    "            validation_split=0.2,  # 원래 vit keras에서는 0.2\n",
    "            callbacks=[\n",
    "                # es, # early_stop 사용할 경우에는 추가할 것\n",
    "                checkpoint_callback],\n",
    "        )\n",
    "\n",
    "        model.load_weights(checkpoint_filepath)\n",
    "        # _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test) # _ : 값을 무시 ref: https://mingrammer.com/underscore-in-python/\n",
    "        v2_test_loss, v2_test_acc = model.evaluate(x_test, y_test)\n",
    "        v2_val_loss, v2_val_acc = model.evaluate(x_val, y_val)\n",
    "\n",
    "        print(f\"v2_val_acc:, {round(v2_val_acc * 100, 2)}%\")\n",
    "        print(f\"v2_test_acc:, {round(v2_test_acc * 100, 2)}%\")\n",
    "\n",
    "        \n",
    "        ################################################################################################################################################################################################################################################################\n",
    "\n",
    "\n",
    "        ################################################################################################################################################################################################################################################################\n",
    "        # probability save\n",
    "        result_pred = list(model.predict(x_test))\n",
    "        pd_result_pred = pd.DataFrame(result_pred)\n",
    "        pd_test_set = pd.DataFrame(test_set)\n",
    "        pd_result_pred_2 = pd.concat([pd_test_set, pd_result_pred], axis=1)\n",
    "        save_dir_probability = '/BiO/Sangjin_Ko_work_2022/16_Vision_Transformer_TEST' + folder + '/Tomato_Checkpoint' + '/Tomato_Iteration_'+ str(idx_iter) + '_PROBABILITY_' + str(now) + \"_&_\" + 'Network_ViT' + '.csv'\n",
    "        print('save_dir_probability:', save_dir_probability)\n",
    "        pd_result_pred_2.to_csv(save_dir_probability, header=True, index=True)\n",
    "        ################################################################################################################################################################################################################################################################\n",
    "\n",
    "\n",
    "        ################################################################################################################################################################################################################################################################\n",
    "        # Confusion matrix\n",
    "        # import matplotlib.pyplot as plt_cnfmatrix\n",
    "        labels_names = [i for i in range(len(Tomato_Dataset_all.class_indices))]\n",
    "        target_names = list(Tomato_Dataset_all.class_indices)\n",
    "        y_pred = model.predict(x_test)\n",
    "\n",
    "        #\n",
    "        y_pred_tolist = y_pred.tolist()\n",
    "        y_test_tolist = y_test.tolist()\n",
    "\n",
    "        # print('y_pred_tolist:', y_pred_tolist)\n",
    "        # print(len(y_pred_tolist))\n",
    "        # print('y_test_tolist', y_test_tolist)\n",
    "        # print(len(y_test_tolist))\n",
    "\n",
    "        y_pred_convert = []\n",
    "        y_test_convert = y_test_tolist\n",
    "        # y_test_convert = []\n",
    "\n",
    "        for idx_convert in range(len(y_pred_tolist)):\n",
    "            y_pred_convert.append(list(y_pred_tolist[idx_convert]).index(max(list(y_pred_tolist[idx_convert]))))\n",
    "\n",
    "        print('y_pred_convert:', y_pred_convert)\n",
    "        print('y_test_convert:', y_test_convert)\n",
    "\n",
    "        cnf_matrix = confusion_matrix(y_test_convert, y_pred_convert, labels=labels_names)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix, display_labels=target_names)\n",
    "        disp = disp.plot(cmap=plt.cm.Blues, values_format='g', xticks_rotation='vertical')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.savefig('/BiO/Sangjin_Ko_work_2022/16_Vision_Transformer_TEST' + folder + '/Tomato_Checkpoint' + '/Tomato_Iteration_' + str(idx_iter) + '_TEST_RESULT_' + str(now) + \"_&_\" + 'Network_ViT' + '.jpg', dpi=400, bbox_inches='tight')\n",
    "        ################################################################################################################################################################################################################################################################\n",
    "\n",
    "\n",
    "        ################################################################################################################################################################################################################################################################    \n",
    "        # leaf classification v2 에서는 idx_model 변수로 CNN 모델간의 정확도 비교했었음\n",
    "        result_acc[idx_iter, idx_model] = v2_test_acc\n",
    "        print('result_acc:', result_acc)\n",
    "\n",
    "        result_val_acc[idx_iter, idx_model] = v2_val_acc\n",
    "        print('result_val_acc:', result_val_acc)\n",
    "\n",
    "        result_precision[idx_iter, idx_model] = precision_score(y_test_convert, y_pred_convert, pos_label=1)\n",
    "        # result_precision[idx_iter, idx_model] = precision_score(y_test_convert, y_pred_convert, average = 'micro', pos_label=1)\n",
    "        print('result_precision:', result_precision)\n",
    "        # average = [None, 'micro', 'macro', 'weighted'] 옵션이 없으면 default = binary 이기 때문에 앞서 나온 4개중에 바꿔줘야함\n",
    "        # ref: https://stackoverflow.com/questions/52269187/facing-valueerror-target-is-multiclass-but-average-binary\n",
    "\n",
    "        result_recall[idx_iter, idx_model] = recall_score(y_test_convert, y_pred_convert, pos_label=1)\n",
    "        # result_precision[idx_iter, idx_model] = precision_score(y_test_convert, y_pred_convert, average = 'micro', pos_label=1)\n",
    "        print('result_recall:', result_recall)\n",
    "        # average = [None, 'micro', 'macro', 'weighted'] 옵션이 없으면 default = binary 이기 때문에 앞서 나온 4개중에 바꿔줘야함\n",
    "        # ref: https://stackoverflow.com/questions/52269187/facing-valueerror-target-is-multiclass-but-average-binary\n",
    "\n",
    "        result_f1[idx_iter, idx_model] = f1_score(y_test_convert, y_pred_convert, pos_label= 1)\n",
    "        # result_precision[idx_iter, idx_model] = precision_score(y_test_convert, y_pred_convert, average = 'micro', pos_label=1)\n",
    "        print('result_f1:', result_f1)\n",
    "        # average = [None, 'micro', 'macro', 'weighted'] 옵션이 없으면 default = binary 이기 때문에 앞서 나온 4개중에 바꿔줘야함\n",
    "        # ref: https://stackoverflow.com/questions/52269187/facing-valueerror-target-is-multiclass-but-average-binary\n",
    "\n",
    "\n",
    "        result_pred_pd[0] = [target_names[y_test_convert[i]] for i in range(len(y_test_convert))]\n",
    "        result_pred_pd[idx_model + 1] = [target_names[y_pred_convert[i]] for i in range(len(y_pred_convert))]\n",
    "\n",
    "        print(time.strftime('%c', time.localtime(time.time())))\n",
    "        print(': Success to make and test model (', (idx_model), \"'s Tomato_model of iteration \", \" \", (idx_iter), \")\")\n",
    "\n",
    "\n",
    "        result_pred_pd.columns = ['True label', 'ViT']\n",
    "        result_pred_pd.index = test_set_new\n",
    "        result_pred_pd.to_excel(writer, sheet_name=('Tomato_Iteration_' + str(idx_iter)))\n",
    "        print('result_pred_pd:', result_pred_pd)\n",
    "        ################################################################################################################################################################################################################################################################\n",
    "\n",
    "\n",
    "        ################################################################################################################################################################################################################################################################    \n",
    "        # 5. 모델 학습과정 표시하기\n",
    "        # import matplotlib.pyplot as plt_matplot\n",
    "\n",
    "        y_range = np.arange(0.0, 1.1, 0.1)\n",
    "        fig, loss_ax = plt.subplots()\n",
    "        acc_ax = loss_ax.twinx()\n",
    "\n",
    "        loss_ax.plot(history.history['loss'], 'y', label = 'train loss')\n",
    "        loss_ax.plot(history.history['val_loss'], 'r', label = 'val loss')\n",
    "\n",
    "        acc_ax.plot(history.history['accuracy'], 'b', label = 'train accuracy')\n",
    "        acc_ax.plot(history.history['val_accuracy'], 'g', label = 'val accuracy')\n",
    "\n",
    "        loss_ax.set_xlabel('epoch')\n",
    "        loss_ax.set_ylabel('loss')\n",
    "        acc_ax.set_ylabel('accuracy')\n",
    "\n",
    "        loss_ax.legend(loc='upper left')\n",
    "        acc_ax.legend(loc='lower left')\n",
    "\n",
    "        loss_ax.set_ylim([0, 1.0])\n",
    "        loss_ax.set_yticks(y_range)\n",
    "\n",
    "        acc_ax.set_ylim([0, 1.0])\n",
    "        acc_ax.set_yticks(y_range)   \n",
    "\n",
    "        plt.savefig('/BiO/Sangjin_Ko_work_2022/16_Vision_Transformer_TEST' + folder + '/Tomato_Checkpoint' + '/Tomato_Iteration_' + str(idx_iter) + '_TRAIN_LOG_' + str(now) + \"_&_\" + 'Network_ViT' + '.png')\n",
    "        print('matplot saved')\n",
    "\n",
    "\n",
    "        ################################################################################################################################################################################################################################################################        \n",
    "        \n",
    "        return history\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbed1f20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T07:07:01.676236Z",
     "start_time": "2023-12-08T05:58:11.473519Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# v2 기반 스크립트\n",
    "\n",
    "iter = 3\n",
    "\n",
    "for idx_iter in range(iter):\n",
    "    print('==========================================', idx_iter, 'idx_iter', '==========================================')\n",
    "\n",
    "    # train_image_value = [] # x_train -> train_image_value\n",
    "    # train_image_categ = [] # y_train -> train_image_categ\n",
    "    # val_image_value = [] # x_val -> val_image_value\n",
    "    # val_image_categ = [] # y_val -> val_image_categ\n",
    "    # test_image_value = [] # x_test -> test_image_value\n",
    "    # test_image_categ = [] # y_test -> test_image_categ\n",
    "\n",
    "\n",
    "    # train/val/test set을 위한 변수\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_val = []\n",
    "    y_val = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    tmp = 0\n",
    "    random_number_val = []\n",
    "    random_number_test = []\n",
    "\n",
    "    #####################################\n",
    "    # train/val/test random number 추출 #\n",
    "    #####################################\n",
    "\n",
    "\n",
    "    # train/val/test random number 추출\n",
    "    # image_all.classes_indices 2가지 종류에서 0: 정상, 1:병징\n",
    "    for idx_categ in range(len(Tomato_Dataset_all.class_indices)):  # len(Tomato_Dataset_all.class_indices -> num_classes(Healthy, Disease) == rankge(2) == [0,1]\n",
    "\n",
    "        # random_num_max = 각 폴더 내 사진의 전체 개수\n",
    "        random_num_max = list(Tomato_Sample_info[:][0]).count(list(Tomato_Dataset_all.class_indices.keys())[idx_categ]) # idx_categ == 0 : Healthy , idx_categ == 1 : Disease\n",
    "        print('정상, 병징 각 폴더 내 사진 전체 개수:', random_num_max)\n",
    "\n",
    "        # random_num_max_val = testset 제외한(0.2, 20%) 사진의 전체 개수(즉 train & val 전체 개수 구함)\n",
    "        random_num_max_val = math.floor(list(Tomato_Sample_info[:][0]).count(list(Tomato_Dataset_all.class_indices.keys())[idx_categ]) * (1 - Ratio_test_set))\n",
    "            # math.ceil = 실수를 올림해 정수로 반환\n",
    "            # math.floor = 실수를 내림해 정수로 반환\n",
    "        print('정상, 병징 각 폴더 내 testset 제외 사진 전체 개수:', random_num_max_val)\n",
    "\n",
    "\n",
    "        ####################################################################################\n",
    "        ##################            Validation set 값들을 지정          ##################\n",
    "        ####################################################################################\n",
    "\n",
    "        # 0 이상 random_num_max_val 미만의 범위에서 Ratio_val_set(0.2, 20%) 만큼의 수를 랜덤하게 샘플링하라\n",
    "        tmp_random_number_val = sample(range(0, random_num_max_val), math.ceil(random_num_max_val * Ratio_val_set)) # Ratio_val_set = 0.8 * 0.2 == 0.16\n",
    "            # sample(컬렉션, 샘플수) : 지정된 컬렉션으로부터 샘플수만큼 리스트 형식으로 랜덤 추출하라\n",
    "            # math.ceil(random_num_max_val * Ratio_val_set) : 전체에서 0.8의 train set에서 0.2의 val 추출\n",
    "        print('random 추출한 validation 용 데이터 전체 개수:', len(tmp_random_number_val))\n",
    "\n",
    "\n",
    "        ####################################################################################\n",
    "        ##################             Test set 값들을 지정               ##################\n",
    "        ####################################################################################\n",
    "        # TESTset은 여러 input datset의 마지막 data들만 가져오므로 일정하다\n",
    "        # tmp_random_number_test = test에 사용될 사진 개수\n",
    "        tmp_random_number_test = list(range(random_num_max_val, random_num_max))\n",
    "        print('전체 데이터 셋의 하위 20% 선별한 test용 데이터 전체 개수:', len(tmp_random_number_test))\n",
    "\n",
    "\n",
    "        ####################################################################################\n",
    "        ####  Validation, test set 값들을 실수 형식으로 누적 시켜 최종 리스트로 뽑는다  ####\n",
    "        ####################################################################################\n",
    "        # Validation set 값들을 실수 형식으로 누적시켜 하나의 리스트로 뽑는다\n",
    "        random_number_val.extend(tmp_random_number_val + np.ones(shape=(len(tmp_random_number_val))) * tmp)\n",
    "\n",
    "        # test set 값들을 실수 형식으로 누적시켜 하나의 리스트로 뽑는다\n",
    "        random_number_test.extend(tmp_random_number_test + np.ones(shape=(len(tmp_random_number_test))) * tmp)\n",
    "\n",
    "        # 각 폴더(Healthy, Disease 내에서의 번호가 아닌 전체 156개 사진에서의 번호 지정을 위함\n",
    "        tmp = tmp + random_num_max\n",
    "\n",
    "    # 파일명 저장을 위한 리스트 변수\n",
    "        # map 함수 == map(함수, 반복 가능 자료형)의 형태를 가짐\n",
    "        # list(map(int, random_number_val)) == 기존 float 형태이던 random_number_val을 int type으로 바꿔 list화\n",
    "    val_set = [list(Tomato_Sample_info[:][1])[i] for i in list(map(int, random_number_val))]\n",
    "    test_set = [list(Tomato_Sample_info[:][1])[i] for i in list(map(int, random_number_test))]\n",
    "    test_set_new = []\n",
    "    val_set_new = []\n",
    "\n",
    "    \n",
    "   \n",
    "    # image sample을 random number에 기반하여 train/val/test split\n",
    "    for idx_all_categ in range(len(Tomato_Dataset_all)):\n",
    "        tmp = Tomato_Dataset_all.filenames[idx_all_categ].split(\"/\")[1]\n",
    "        tmp_value = cv2.imread(os.path.join(Tomato_dataset_path, Tomato_Dataset_all.filenames[idx_all_categ]))\n",
    "        # 이전 스크립트: img_value = cv2.imread(Tomato_dataset_path + '/' + Tomato_Dataset_all.filenames[img_num])/255\n",
    "        # print('reshape 이전 tmp_value:', tmp_value)\n",
    "\n",
    "        tmp_value = tmp_value.reshape(1, tmp_value.shape[0], tmp_value.shape[1], tmp_value.shape[2])\n",
    "            # {numpy.array}.shape == numpy.array가 어떤 모습 및 차원으로 구성되는지 보여줌\n",
    "            # (1, 256, 256, 3) 형태로 reshape\n",
    "        # print(tmp_value.shape) # (1, 256, 256, 3)\n",
    "\n",
    "        tmp_categ = to_categorical(Tomato_Dataset_all.classes)[idx_all_categ]\n",
    "            # to_categorical == 정수형 클래스 레이블을 원-핫 인코딩 벡터로 변환하는 함수\n",
    "            # Tomato_Dataset_all.classes == 0, 1로 작물의 정상, 병징에 대한 category를 모두 정리해 둔 리스트\n",
    "            # idx_all_categ == 전체 category의 index를 불러옴(110번째, 111번째 ,,,)\n",
    "        # print('before reshape tmp_categ:', tmp_categ) # tmp_categ = [1. 0.] or [0. 1.]\n",
    "\n",
    "        tmp_categ = tmp_categ.reshape(1, tmp_categ.shape[0])\n",
    "        # print('after reshape tmp_categ:', tmp_categ) # (after reshape)tmp_categ == [[1. 0.]] of [[0. 1.]]\n",
    "        # print('after reshape tmp_categ.shape', tmp_categ.shape) # (after reshape)tmp_categ.shape == (1,2)\n",
    "\n",
    "\n",
    "        if tmp in val_set: # val_set == validation 이미지 파일명들 모아둔 리스트\n",
    "            # if Tomato_Dataset_all.filenames[idx_all_categ].split(\"/\")[1].split(\".\")[0] == '1':\n",
    "                # 원본 leaf_classification에서는 조건이 있었던 것 같지만 현재는 없으므로 패스\n",
    "            x_val.append(tmp_value)\n",
    "            y_val.append(tmp_categ)\n",
    "            val_set_new.append(tmp) # 왜 val_set_new에 새로 validation set을 만들지?\n",
    "\n",
    "        elif tmp in test_set: # test_set == test 이미지 파일명들 모아둔\" 리스트\n",
    "            # if Tomato_Dataset_all.filenames[idx_all_categ].split(\"/\")[1].split(\".\")[0] == '1':\n",
    "                # 원본 leaf_classification에서는 조건이 있었던 것 같지만 현재는 없으므로 패스\n",
    "            x_test.append(tmp_value)\n",
    "            y_test.append(tmp_categ)\n",
    "            test_set_new.append(tmp) # 왜 test_set_new에 새로 validation set을 만들지?\n",
    "\n",
    "        else:\n",
    "            x_train.append(tmp_value)\n",
    "            y_train.append(tmp_categ) # 왜 test_set_new에 새로 validation set을 만들지?\n",
    "\n",
    "\n",
    "    print(\": Number of dataset(Train -\", str(len(x_train)), \"/Val -\", str(len(val_set_new)), \"/Test -\", str(len(test_set_new)), \")\")\n",
    "\n",
    "    with tf.device('/GPU:0'): \n",
    "        # np.array(x_train).shape == (2518, 1, 256, 256, 3)\n",
    "        x_train = np.array(x_train).reshape(np.array(x_train).shape[0], np.array(x_train).shape[2], np.array(x_train).shape[3], np.array(x_train).shape[4])\n",
    "        y_train = np.array(y_train).reshape(np.array(y_train).shape[0], np.array(y_train).shape[2])\n",
    "        x_val = np.array(x_val).reshape(np.array(x_val).shape[0], np.array(x_val).shape[2], np.array(x_val).shape[3], np.array(x_val).shape[4])\n",
    "        y_val = np.array(y_val).reshape(np.array(y_val).shape[0], np.array(y_val).shape[2])\n",
    "        x_test = np.array(x_test).reshape(np.array(x_test).shape[0], np.array(x_test).shape[2], np.array(x_test).shape[3], np.array(x_test).shape[4])\n",
    "        y_test = np.array(y_test).reshape(np.array(y_test).shape[0], np.array(y_test).shape[2])\n",
    "\n",
    "        print('x_train.shape:',x_train.shape)\n",
    "        print('y_train.shape:',y_train.shape)\n",
    "        print('x_val.shape:',x_val.shape)\n",
    "        print('y_val.shape:',y_val.shape)\n",
    "        print('x_test.shape:',x_test.shape)\n",
    "        print('y_test.shape:',y_test.shape)\n",
    "\n",
    "        print(\": Success to split dataset\")\n",
    "\n",
    "    # np.argmax\n",
    "    y_train = np.argmax(y_train, axis=1)\n",
    "    y_val = np.argmax(y_val, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    print('np.argmax(y_train, axis =1):', y_train.shape)\n",
    "    print('np.argmax(y_val, axis=1):', y_val.shape)\n",
    "    print('np.argmax(y_test, axis=1):', y_test.shape)\n",
    "\n",
    "\n",
    "    # result_pred_pd 생성\n",
    "    result_pred_pd = pd.DataFrame(np.zeros(shape=(len(y_test), 2)))\n",
    "    num_categ = len(Tomato_Dataset_all.class_indices)\n",
    "\n",
    "\n",
    "    # 원본 data augmentation -> data resizing\n",
    "    data_resizing = keras.Sequential(\n",
    "        [\n",
    "            layers.Normalization(),\n",
    "            layers.Resizing(image_size, image_size),\n",
    "        ],\n",
    "        name=\"data_resizing\",\n",
    "    )\n",
    "    # Compute the mean and the variance of the training data for normalization.\n",
    "    data_resizing.layers[0].adapt(x_train)\n",
    "    print('data_resizing.layers[0].adapt(x_train):', data_resizing.layers[0].adapt(x_train))\n",
    "\n",
    "    \n",
    "    # v2기반 스크립트\n",
    "    # Display patches for a sample image\n",
    "    import matplotlib.pyplot as plt_imshow   \n",
    "\n",
    "    plt_imshow.imshow(x_train[0])\n",
    "\n",
    "    plt_imshow.figure(figsize=(4, 4))\n",
    "    image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "\n",
    "    # 원본 스크립트\n",
    "    # plt_imshow.imshow(image.astype(\"uint8\")) # uint8 type으로 바꾸면 검은색 사진으로만 나옴\n",
    "    plt_imshow.imshow(image.astype(\"uint8\"))\n",
    "    plt_imshow.axis(\"off\")\n",
    "    resized_image = tf.image.resize(\n",
    "        tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    "    )\n",
    "    patches = Patches(patch_size)(resized_image)\n",
    "    print(f\"Image size: {image_size} X {image_size}\")\n",
    "    print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "    print(f\"Patches per image: {patches.shape[1]}\")\n",
    "    print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "    n = int(np.sqrt(patches.shape[1]))\n",
    "    plt_imshow.figure(figsize=(4, 4))\n",
    "    for i, patch in enumerate(patches[0]):\n",
    "        ax = plt_imshow.subplot(n, n, i + 1)\n",
    "        patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "        plt_imshow.imshow(patch_img.numpy().astype(\"uint8\")) # unit8 type으로 바꾸면 검은색 사진으로만 나옴\n",
    "        plt_imshow.axis(\"off\")\n",
    "\n",
    "    print('patch 크기',np.array(patch_img).shape)\n",
    "\n",
    "\n",
    "    # 학습 수행\n",
    "    vit_classifier = create_vit_classifier()\n",
    "    history = run_experiment(vit_classifier)\n",
    "\n",
    "\n",
    "# Accuracy, precision, recall, f1 score 구하고 excel 파일에 저장\n",
    "result_acc_pd = pd.DataFrame(result_acc, columns=['ViT'])\n",
    "result_val_acc_pd = pd.DataFrame(result_val_acc, columns=['ViT'])\n",
    "\n",
    "result_acc_pd.to_excel(writer, sheet_name='summary_test_set')\n",
    "result_val_acc_pd.to_excel(writer, sheet_name='summary_val_set')\n",
    "\n",
    "result_precision_pd = pd.DataFrame(result_precision, columns=['ViT'])\n",
    "result_recall_pd = pd.DataFrame(result_recall, columns=['ViT'])\n",
    "result_f1_pd = pd.DataFrame(result_f1, columns=['ViT'])\n",
    "\n",
    "result_precision_pd.to_excel(writer, sheet_name='Precision')\n",
    "result_recall_pd.to_excel(writer, sheet_name='Recall')\n",
    "result_f1_pd.to_excel(writer, sheet_name='f1 score')\n",
    "\n",
    "writer.save()\n",
    "\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29a15c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T07:29:18.060486Z",
     "start_time": "2023-12-01T07:29:17.877690Z"
    }
   },
   "outputs": [],
   "source": [
    "vit_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d33ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
